{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoon0gim/Dacon/blob/main/Project01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b70f44",
      "metadata": {
        "id": "b6b70f44"
      },
      "source": [
        "## 상의, 하의 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45be238b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45be238b",
        "outputId": "62f2f1c8-fc6a-4a82-d0ec-8aea923ac1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "from glob import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "#print(os.getcwd())\n",
        "os.chdir('/content')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q60EmRMyCQFi",
        "outputId": "345bd7d6-5d49-45ed-b32f-eb22c1c2abfc"
      },
      "id": "Q60EmRMyCQFi",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('./gdrive/My Drive/user_data/train.csv')\n",
        "\n",
        "train_data.head()\n",
        "\n",
        "train_df = train_data.replace('10-1', 10)\n",
        "train_df2 = train_df.replace('10-2', 11)\n",
        "train_df2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nDfKuuKTFipy",
        "outputId": "99f081b0-4f87-4c0b-9921-8557c76eff5d"
      },
      "id": "nDfKuuKTFipy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  file_name label\n",
              "0   001.png    11\n",
              "1   002.png    10\n",
              "2   003.png     3\n",
              "3   004.png     8\n",
              "4   005.png     9"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57069b50-d7de-4739-bb78-54648f9ab15c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.png</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.png</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004.png</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005.png</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57069b50-d7de-4739-bb78-54648f9ab15c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57069b50-d7de-4739-bb78-54648f9ab15c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57069b50-d7de-4739-bb78-54648f9ab15c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1c954b52",
      "metadata": {
        "id": "1c954b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4f96684-0f36-4c73-82dc-46386e30e8b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./gdrive/My Drive/user_data/train/004.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "data_list = glob('./gdrive/My Drive/user_data/train/*.png')\n",
        "data_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed3eb67e",
      "metadata": {
        "id": "ed3eb67e",
        "outputId": "97813382-589a-4cc3-87f0-ed307d40ff04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['whole', 'whole', '1', '1', 'png']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "#text_to_word_sequence(data_list[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72f0ee4",
      "metadata": {
        "id": "c72f0ee4"
      },
      "outputs": [],
      "source": [
        "#label = []\n",
        "#for i in range(len(data_list)):\n",
        "#    token = text_to_word_sequence(data_list[i])\n",
        "#    label.append(token[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c7cdc9",
      "metadata": {
        "id": "d1c7cdc9",
        "outputId": "d6fed604-1b04-485d-9aa2-ac1bd59385d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'bottom', 'bottom', 'bottom', 'bottom',\n",
              "       'bottom', 'bottom', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe',\n",
              "       'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'shoe', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top', 'top',\n",
              "       'top', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole', 'whole', 'whole', 'whole', 'whole', 'whole',\n",
              "       'whole', 'whole'], dtype='<U6')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "items = label\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "label = encoder.transform(items)\n",
        "\n",
        "# lebel 디코딩 확인한다\n",
        "encoder.inverse_transform(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2b2145f6",
      "metadata": {
        "scrolled": false,
        "id": "2b2145f6"
      },
      "outputs": [],
      "source": [
        "# cv2 모듈을 불러 온다. \n",
        "import cv2\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "012d461a",
      "metadata": {
        "id": "012d461a"
      },
      "outputs": [],
      "source": [
        "data_height = 150\n",
        "data_width = 150\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6f6eef61",
      "metadata": {
        "id": "6f6eef61"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import cv2\n",
        "\n",
        "\n",
        "# image initialization\n",
        "data_height = 150\n",
        "data_width = 150\n",
        "\n",
        "batch_size = len(data_list)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# data processing 함수 \n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "def make_file(data_height, data_width, batch_size):\n",
        "    label = []\n",
        "    images = np.zeros((batch_size, data_height, data_width))\n",
        "    for n, path in enumerate(data_list[:batch_size]):\n",
        "\n",
        "    # image transform\n",
        "        image = cv2.imread(data_list[n], 0)\n",
        "        image = cv2.resize(image, (data_height, data_width), interpolation=cv2.INTER_LINEAR)/255\n",
        "        images[n, :, :] =image\n",
        "    \n",
        "    label = np.array(label)\n",
        "        \n",
        "    return (label, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62b0a674",
      "metadata": {
        "id": "62b0a674",
        "outputId": "e3625a3a-969f-4090-c906-4e70fa13eceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(858, 150, 150) (0,)\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "(label, images) = make_file(data_height, data_width, batch_size)\n",
        "\n",
        "# 이미지 데이터 구조와 라벨 구조를 보기\n",
        "print(images.shape, label.shape)\n",
        "#print(label)\n",
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bb938dea",
      "metadata": {
        "id": "bb938dea",
        "outputId": "4ec6783b-7103-401c-a7ad-090134ad3a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# data label encoder\n",
        "\n",
        "items = train_df2['label']\n",
        "#encoder = LabelEncoder()\n",
        "#encoder.fit(items)\n",
        "#label = encoder.transform(items)\n",
        "#print(label)\n",
        "label = tf.keras.utils.to_categorical(items)\n",
        "print(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label.shape"
      ],
      "metadata": {
        "id": "lEpzw2wqKoOk",
        "outputId": "24328390-39fb-4cfb-903a-637005e4a2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lEpzw2wqKoOk",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(858, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9e4f4284",
      "metadata": {
        "id": "9e4f4284",
        "outputId": "08d6f00c-27e1-435b-fc26-517d08313e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 150, 150) (258, 150, 150)\n",
            "(600, 12) (258, 12)\n",
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "# data split train set, test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x = images \n",
        "y = label\n",
        "\n",
        "#y_train = np_utils.to_categorical(y, 4)\n",
        "#y_test = np_utils.to_categorical(y, 4)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, \n",
        "                                                    y, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=1004)\n",
        "\n",
        "# 데이터 셋이  나누어진 것에 대한 구조보기\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2e9c58ed",
      "metadata": {
        "id": "2e9c58ed",
        "outputId": "886031b3-23a6-4929-cefe-4ecf45dc3ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 32)      320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 34, 34, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 147968)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                3551256   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 24)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 12)                300       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,644,228\n",
            "Trainable params: 3,644,228\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "  # CNN model network\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(data_height, data_width, 1)))\n",
        "model.add(layers.MaxPooling2D(pool_size=2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=2))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(24, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(12, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9593f32b",
      "metadata": {
        "id": "9593f32b",
        "outputId": "19928ec8-e9cc-4629-92cd-7a49a7d34ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.5063 - accuracy: 0.0746\n",
            "Epoch 1: val_loss improved from inf to 2.47069, saving model to ./model/01-2.4707.hdf5\n",
            "60/60 [==============================] - 14s 50ms/step - loss: 2.5053 - accuracy: 0.0750 - val_loss: 2.4707 - val_accuracy: 0.0891\n",
            "Epoch 2/1000\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4818 - accuracy: 0.0833\n",
            "Epoch 2: val_loss did not improve from 2.47069\n",
            "60/60 [==============================] - 2s 38ms/step - loss: 2.4818 - accuracy: 0.0833 - val_loss: 2.4740 - val_accuracy: 0.0891\n",
            "Epoch 3/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4724 - accuracy: 0.0932\n",
            "Epoch 3: val_loss improved from 2.47069 to 2.46315, saving model to ./model/03-2.4631.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4719 - accuracy: 0.0933 - val_loss: 2.4631 - val_accuracy: 0.0814\n",
            "Epoch 4/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4690 - accuracy: 0.0780\n",
            "Epoch 4: val_loss improved from 2.46315 to 2.45784, saving model to ./model/04-2.4578.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4686 - accuracy: 0.0767 - val_loss: 2.4578 - val_accuracy: 0.0853\n",
            "Epoch 5/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4548 - accuracy: 0.1136\n",
            "Epoch 5: val_loss improved from 2.45784 to 2.44456, saving model to ./model/05-2.4446.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4566 - accuracy: 0.1117 - val_loss: 2.4446 - val_accuracy: 0.0891\n",
            "Epoch 6/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4567 - accuracy: 0.1085\n",
            "Epoch 6: val_loss did not improve from 2.44456\n",
            "60/60 [==============================] - 2s 31ms/step - loss: 2.4568 - accuracy: 0.1100 - val_loss: 2.4449 - val_accuracy: 0.0736\n",
            "Epoch 7/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4502 - accuracy: 0.0915\n",
            "Epoch 7: val_loss did not improve from 2.44456\n",
            "60/60 [==============================] - 2s 32ms/step - loss: 2.4502 - accuracy: 0.0900 - val_loss: 2.4556 - val_accuracy: 0.0930\n",
            "Epoch 8/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4539 - accuracy: 0.0814\n",
            "Epoch 8: val_loss did not improve from 2.44456\n",
            "60/60 [==============================] - 2s 31ms/step - loss: 2.4537 - accuracy: 0.0817 - val_loss: 2.4520 - val_accuracy: 0.0891\n",
            "Epoch 9/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4501 - accuracy: 0.0797\n",
            "Epoch 9: val_loss did not improve from 2.44456\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4502 - accuracy: 0.0800 - val_loss: 2.4488 - val_accuracy: 0.0891\n",
            "Epoch 10/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4472 - accuracy: 0.0983\n",
            "Epoch 10: val_loss did not improve from 2.44456\n",
            "60/60 [==============================] - 2s 32ms/step - loss: 2.4471 - accuracy: 0.0983 - val_loss: 2.4460 - val_accuracy: 0.0891\n",
            "Epoch 11/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4442 - accuracy: 0.1000\n",
            "Epoch 11: val_loss improved from 2.44456 to 2.44311, saving model to ./model/11-2.4431.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4442 - accuracy: 0.0983 - val_loss: 2.4431 - val_accuracy: 0.0891\n",
            "Epoch 12/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4417 - accuracy: 0.0966\n",
            "Epoch 12: val_loss improved from 2.44311 to 2.44087, saving model to ./model/12-2.4409.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4415 - accuracy: 0.0983 - val_loss: 2.4409 - val_accuracy: 0.0891\n",
            "Epoch 13/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4394 - accuracy: 0.0966\n",
            "Epoch 13: val_loss improved from 2.44087 to 2.43863, saving model to ./model/13-2.4386.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4392 - accuracy: 0.0983 - val_loss: 2.4386 - val_accuracy: 0.0891\n",
            "Epoch 14/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4440 - accuracy: 0.0932\n",
            "Epoch 14: val_loss improved from 2.43863 to 2.43022, saving model to ./model/14-2.4302.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4432 - accuracy: 0.0950 - val_loss: 2.4302 - val_accuracy: 0.0736\n",
            "Epoch 15/1000\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4327 - accuracy: 0.0900\n",
            "Epoch 15: val_loss improved from 2.43022 to 2.42967, saving model to ./model/15-2.4297.hdf5\n",
            "60/60 [==============================] - 2s 36ms/step - loss: 2.4327 - accuracy: 0.0900 - val_loss: 2.4297 - val_accuracy: 0.0891\n",
            "Epoch 16/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4356 - accuracy: 0.0932\n",
            "Epoch 16: val_loss did not improve from 2.42967\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4353 - accuracy: 0.0933 - val_loss: 2.4322 - val_accuracy: 0.0775\n",
            "Epoch 17/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4321 - accuracy: 0.0831\n",
            "Epoch 17: val_loss did not improve from 2.42967\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4316 - accuracy: 0.0817 - val_loss: 2.4316 - val_accuracy: 0.0891\n",
            "Epoch 18/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4300 - accuracy: 0.1000\n",
            "Epoch 18: val_loss did not improve from 2.42967\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4300 - accuracy: 0.0983 - val_loss: 2.4299 - val_accuracy: 0.0891\n",
            "Epoch 19/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4288 - accuracy: 0.0983\n",
            "Epoch 19: val_loss improved from 2.42967 to 2.42845, saving model to ./model/19-2.4284.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4284 - accuracy: 0.0983 - val_loss: 2.4284 - val_accuracy: 0.0891\n",
            "Epoch 20/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4268 - accuracy: 0.0983\n",
            "Epoch 20: val_loss improved from 2.42845 to 2.42695, saving model to ./model/20-2.4269.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4268 - accuracy: 0.0967 - val_loss: 2.4269 - val_accuracy: 0.0891\n",
            "Epoch 21/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4256 - accuracy: 0.0898\n",
            "Epoch 21: val_loss improved from 2.42695 to 2.42563, saving model to ./model/21-2.4256.hdf5\n",
            "60/60 [==============================] - 2s 36ms/step - loss: 2.4255 - accuracy: 0.0883 - val_loss: 2.4256 - val_accuracy: 0.0891\n",
            "Epoch 22/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4245 - accuracy: 0.0915\n",
            "Epoch 22: val_loss improved from 2.42563 to 2.42425, saving model to ./model/22-2.4242.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4241 - accuracy: 0.0950 - val_loss: 2.4242 - val_accuracy: 0.0891\n",
            "Epoch 23/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4231 - accuracy: 0.0847\n",
            "Epoch 23: val_loss improved from 2.42425 to 2.42313, saving model to ./model/23-2.4231.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4228 - accuracy: 0.0850 - val_loss: 2.4231 - val_accuracy: 0.0891\n",
            "Epoch 24/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4217 - accuracy: 0.0864\n",
            "Epoch 24: val_loss improved from 2.42313 to 2.42203, saving model to ./model/24-2.4220.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4218 - accuracy: 0.0867 - val_loss: 2.4220 - val_accuracy: 0.0891\n",
            "Epoch 25/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4198 - accuracy: 0.1017\n",
            "Epoch 25: val_loss improved from 2.42203 to 2.42092, saving model to ./model/25-2.4209.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4206 - accuracy: 0.1000 - val_loss: 2.4209 - val_accuracy: 0.0891\n",
            "Epoch 26/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4199 - accuracy: 0.0983\n",
            "Epoch 26: val_loss improved from 2.42092 to 2.42001, saving model to ./model/26-2.4200.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4195 - accuracy: 0.1000 - val_loss: 2.4200 - val_accuracy: 0.0891\n",
            "Epoch 27/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4178 - accuracy: 0.1017\n",
            "Epoch 27: val_loss improved from 2.42001 to 2.41901, saving model to ./model/27-2.4190.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4186 - accuracy: 0.1000 - val_loss: 2.4190 - val_accuracy: 0.0891\n",
            "Epoch 28/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4170 - accuracy: 0.1000\n",
            "Epoch 28: val_loss improved from 2.41901 to 2.41811, saving model to ./model/28-2.4181.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4175 - accuracy: 0.1000 - val_loss: 2.4181 - val_accuracy: 0.0891\n",
            "Epoch 29/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4169 - accuracy: 0.1000\n",
            "Epoch 29: val_loss improved from 2.41811 to 2.41731, saving model to ./model/29-2.4173.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4166 - accuracy: 0.1000 - val_loss: 2.4173 - val_accuracy: 0.0891\n",
            "Epoch 30/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4160 - accuracy: 0.0966\n",
            "Epoch 30: val_loss improved from 2.41731 to 2.41646, saving model to ./model/30-2.4165.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4158 - accuracy: 0.1000 - val_loss: 2.4165 - val_accuracy: 0.0891\n",
            "Epoch 31/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4148 - accuracy: 0.1017\n",
            "Epoch 31: val_loss improved from 2.41646 to 2.41567, saving model to ./model/31-2.4157.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4152 - accuracy: 0.1000 - val_loss: 2.4157 - val_accuracy: 0.0891\n",
            "Epoch 32/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4146 - accuracy: 0.1017\n",
            "Epoch 32: val_loss improved from 2.41567 to 2.41502, saving model to ./model/32-2.4150.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4143 - accuracy: 0.1000 - val_loss: 2.4150 - val_accuracy: 0.0891\n",
            "Epoch 33/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4134 - accuracy: 0.1000\n",
            "Epoch 33: val_loss improved from 2.41502 to 2.41430, saving model to ./model/33-2.4143.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4135 - accuracy: 0.1000 - val_loss: 2.4143 - val_accuracy: 0.0891\n",
            "Epoch 34/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4132 - accuracy: 0.1000\n",
            "Epoch 34: val_loss improved from 2.41430 to 2.41367, saving model to ./model/34-2.4137.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4128 - accuracy: 0.1000 - val_loss: 2.4137 - val_accuracy: 0.0891\n",
            "Epoch 35/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4128 - accuracy: 0.0966\n",
            "Epoch 35: val_loss improved from 2.41367 to 2.41293, saving model to ./model/35-2.4129.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4122 - accuracy: 0.1000 - val_loss: 2.4129 - val_accuracy: 0.0891\n",
            "Epoch 36/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4121 - accuracy: 0.0983\n",
            "Epoch 36: val_loss improved from 2.41293 to 2.41239, saving model to ./model/36-2.4124.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4116 - accuracy: 0.1000 - val_loss: 2.4124 - val_accuracy: 0.0891\n",
            "Epoch 37/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4109 - accuracy: 0.0983\n",
            "Epoch 37: val_loss improved from 2.41239 to 2.41182, saving model to ./model/37-2.4118.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4109 - accuracy: 0.1000 - val_loss: 2.4118 - val_accuracy: 0.0891\n",
            "Epoch 38/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4100 - accuracy: 0.1017\n",
            "Epoch 38: val_loss improved from 2.41182 to 2.41120, saving model to ./model/38-2.4112.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4103 - accuracy: 0.1000 - val_loss: 2.4112 - val_accuracy: 0.0891\n",
            "Epoch 39/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4099 - accuracy: 0.0949\n",
            "Epoch 39: val_loss improved from 2.41120 to 2.41060, saving model to ./model/39-2.4106.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4098 - accuracy: 0.1000 - val_loss: 2.4106 - val_accuracy: 0.0891\n",
            "Epoch 40/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4089 - accuracy: 0.1017\n",
            "Epoch 40: val_loss improved from 2.41060 to 2.41029, saving model to ./model/40-2.4103.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4093 - accuracy: 0.1000 - val_loss: 2.4103 - val_accuracy: 0.0891\n",
            "Epoch 41/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4093 - accuracy: 0.0983\n",
            "Epoch 41: val_loss improved from 2.41029 to 2.40957, saving model to ./model/41-2.4096.hdf5\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 2.4087 - accuracy: 0.1000 - val_loss: 2.4096 - val_accuracy: 0.0891\n",
            "Epoch 42/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4086 - accuracy: 0.1000\n",
            "Epoch 42: val_loss improved from 2.40957 to 2.40919, saving model to ./model/42-2.4092.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4083 - accuracy: 0.1000 - val_loss: 2.4092 - val_accuracy: 0.0891\n",
            "Epoch 43/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4081 - accuracy: 0.1017\n",
            "Epoch 43: val_loss improved from 2.40919 to 2.40880, saving model to ./model/43-2.4088.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4079 - accuracy: 0.1000 - val_loss: 2.4088 - val_accuracy: 0.0891\n",
            "Epoch 44/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4071 - accuracy: 0.1000\n",
            "Epoch 44: val_loss improved from 2.40880 to 2.40833, saving model to ./model/44-2.4083.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4074 - accuracy: 0.1000 - val_loss: 2.4083 - val_accuracy: 0.0891\n",
            "Epoch 45/1000\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4071 - accuracy: 0.1000\n",
            "Epoch 45: val_loss improved from 2.40833 to 2.40779, saving model to ./model/45-2.4078.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4071 - accuracy: 0.1000 - val_loss: 2.4078 - val_accuracy: 0.0891\n",
            "Epoch 46/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4065 - accuracy: 0.1017\n",
            "Epoch 46: val_loss improved from 2.40779 to 2.40754, saving model to ./model/46-2.4075.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4066 - accuracy: 0.1000 - val_loss: 2.4075 - val_accuracy: 0.0891\n",
            "Epoch 47/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4057 - accuracy: 0.1000\n",
            "Epoch 47: val_loss improved from 2.40754 to 2.40716, saving model to ./model/47-2.4072.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4061 - accuracy: 0.1000 - val_loss: 2.4072 - val_accuracy: 0.0891\n",
            "Epoch 48/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4050 - accuracy: 0.1017\n",
            "Epoch 48: val_loss improved from 2.40716 to 2.40677, saving model to ./model/48-2.4068.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4058 - accuracy: 0.1000 - val_loss: 2.4068 - val_accuracy: 0.0891\n",
            "Epoch 49/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4060 - accuracy: 0.1000\n",
            "Epoch 49: val_loss improved from 2.40677 to 2.40643, saving model to ./model/49-2.4064.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4055 - accuracy: 0.1000 - val_loss: 2.4064 - val_accuracy: 0.0891\n",
            "Epoch 50/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4053 - accuracy: 0.0983\n",
            "Epoch 50: val_loss improved from 2.40643 to 2.40596, saving model to ./model/50-2.4060.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4051 - accuracy: 0.1000 - val_loss: 2.4060 - val_accuracy: 0.0891\n",
            "Epoch 51/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4042 - accuracy: 0.1000\n",
            "Epoch 51: val_loss improved from 2.40596 to 2.40574, saving model to ./model/51-2.4057.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4047 - accuracy: 0.1000 - val_loss: 2.4057 - val_accuracy: 0.0891\n",
            "Epoch 52/1000\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4045 - accuracy: 0.1000\n",
            "Epoch 52: val_loss improved from 2.40574 to 2.40536, saving model to ./model/52-2.4054.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4045 - accuracy: 0.1000 - val_loss: 2.4054 - val_accuracy: 0.0891\n",
            "Epoch 53/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4045 - accuracy: 0.1017\n",
            "Epoch 53: val_loss improved from 2.40536 to 2.40507, saving model to ./model/53-2.4051.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4040 - accuracy: 0.1000 - val_loss: 2.4051 - val_accuracy: 0.0891\n",
            "Epoch 54/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4042 - accuracy: 0.1017\n",
            "Epoch 54: val_loss improved from 2.40507 to 2.40471, saving model to ./model/54-2.4047.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4038 - accuracy: 0.1000 - val_loss: 2.4047 - val_accuracy: 0.0891\n",
            "Epoch 55/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4038 - accuracy: 0.1017\n",
            "Epoch 55: val_loss improved from 2.40471 to 2.40449, saving model to ./model/55-2.4045.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4036 - accuracy: 0.1000 - val_loss: 2.4045 - val_accuracy: 0.0891\n",
            "Epoch 56/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4027 - accuracy: 0.1017\n",
            "Epoch 56: val_loss improved from 2.40449 to 2.40433, saving model to ./model/56-2.4043.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4031 - accuracy: 0.1000 - val_loss: 2.4043 - val_accuracy: 0.0891\n",
            "Epoch 57/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4021 - accuracy: 0.1000\n",
            "Epoch 57: val_loss improved from 2.40433 to 2.40399, saving model to ./model/57-2.4040.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4029 - accuracy: 0.1000 - val_loss: 2.4040 - val_accuracy: 0.0891\n",
            "Epoch 58/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4017 - accuracy: 0.1000\n",
            "Epoch 58: val_loss improved from 2.40399 to 2.40365, saving model to ./model/58-2.4036.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4026 - accuracy: 0.1000 - val_loss: 2.4036 - val_accuracy: 0.0891\n",
            "Epoch 59/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4025 - accuracy: 0.1000\n",
            "Epoch 59: val_loss improved from 2.40365 to 2.40351, saving model to ./model/59-2.4035.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4024 - accuracy: 0.1000 - val_loss: 2.4035 - val_accuracy: 0.0891\n",
            "Epoch 60/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4026 - accuracy: 0.1000\n",
            "Epoch 60: val_loss improved from 2.40351 to 2.40322, saving model to ./model/60-2.4032.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4021 - accuracy: 0.1000 - val_loss: 2.4032 - val_accuracy: 0.0891\n",
            "Epoch 61/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4016 - accuracy: 0.1017\n",
            "Epoch 61: val_loss improved from 2.40322 to 2.40299, saving model to ./model/61-2.4030.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4020 - accuracy: 0.1000 - val_loss: 2.4030 - val_accuracy: 0.0891\n",
            "Epoch 62/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4022 - accuracy: 0.1017\n",
            "Epoch 62: val_loss improved from 2.40299 to 2.40271, saving model to ./model/62-2.4027.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4017 - accuracy: 0.1000 - val_loss: 2.4027 - val_accuracy: 0.0891\n",
            "Epoch 63/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4014 - accuracy: 0.0983\n",
            "Epoch 63: val_loss improved from 2.40271 to 2.40246, saving model to ./model/63-2.4025.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4015 - accuracy: 0.1000 - val_loss: 2.4025 - val_accuracy: 0.0891\n",
            "Epoch 64/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4009 - accuracy: 0.1017\n",
            "Epoch 64: val_loss improved from 2.40246 to 2.40233, saving model to ./model/64-2.4023.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.4012 - accuracy: 0.1000 - val_loss: 2.4023 - val_accuracy: 0.0891\n",
            "Epoch 65/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4007 - accuracy: 0.1017\n",
            "Epoch 65: val_loss improved from 2.40233 to 2.40209, saving model to ./model/65-2.4021.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4010 - accuracy: 0.1000 - val_loss: 2.4021 - val_accuracy: 0.0891\n",
            "Epoch 66/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4012 - accuracy: 0.1017\n",
            "Epoch 66: val_loss improved from 2.40209 to 2.40189, saving model to ./model/66-2.4019.hdf5\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 2.4007 - accuracy: 0.1000 - val_loss: 2.4019 - val_accuracy: 0.0891\n",
            "Epoch 67/1000\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4006 - accuracy: 0.1000\n",
            "Epoch 67: val_loss improved from 2.40189 to 2.40174, saving model to ./model/67-2.4017.hdf5\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 2.4006 - accuracy: 0.1000 - val_loss: 2.4017 - val_accuracy: 0.0891\n",
            "Epoch 68/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4004 - accuracy: 0.1017\n",
            "Epoch 68: val_loss improved from 2.40174 to 2.40152, saving model to ./model/68-2.4015.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4005 - accuracy: 0.1000 - val_loss: 2.4015 - val_accuracy: 0.0891\n",
            "Epoch 69/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4002 - accuracy: 0.1000\n",
            "Epoch 69: val_loss improved from 2.40152 to 2.40128, saving model to ./model/69-2.4013.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.4004 - accuracy: 0.1000 - val_loss: 2.4013 - val_accuracy: 0.0891\n",
            "Epoch 70/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4005 - accuracy: 0.1000\n",
            "Epoch 70: val_loss improved from 2.40128 to 2.40117, saving model to ./model/70-2.4012.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.4002 - accuracy: 0.1000 - val_loss: 2.4012 - val_accuracy: 0.0891\n",
            "Epoch 71/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4004 - accuracy: 0.0983\n",
            "Epoch 71: val_loss improved from 2.40117 to 2.40100, saving model to ./model/71-2.4010.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3998 - accuracy: 0.1000 - val_loss: 2.4010 - val_accuracy: 0.0891\n",
            "Epoch 72/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.4002 - accuracy: 0.1000\n",
            "Epoch 72: val_loss did not improve from 2.40100\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3998 - accuracy: 0.1000 - val_loss: 2.4010 - val_accuracy: 0.0891\n",
            "Epoch 73/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3986 - accuracy: 0.1017\n",
            "Epoch 73: val_loss improved from 2.40100 to 2.40060, saving model to ./model/73-2.4006.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3996 - accuracy: 0.1000 - val_loss: 2.4006 - val_accuracy: 0.0891\n",
            "Epoch 74/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3990 - accuracy: 0.1017\n",
            "Epoch 74: val_loss improved from 2.40060 to 2.40055, saving model to ./model/74-2.4006.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3994 - accuracy: 0.1000 - val_loss: 2.4006 - val_accuracy: 0.0891\n",
            "Epoch 75/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3991 - accuracy: 0.1017\n",
            "Epoch 75: val_loss improved from 2.40055 to 2.40036, saving model to ./model/75-2.4004.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.3992 - accuracy: 0.1000 - val_loss: 2.4004 - val_accuracy: 0.0891\n",
            "Epoch 76/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3992 - accuracy: 0.1017\n",
            "Epoch 76: val_loss improved from 2.40036 to 2.40021, saving model to ./model/76-2.4002.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3991 - accuracy: 0.1000 - val_loss: 2.4002 - val_accuracy: 0.0891\n",
            "Epoch 77/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3996 - accuracy: 0.1000\n",
            "Epoch 77: val_loss improved from 2.40021 to 2.40012, saving model to ./model/77-2.4001.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.3991 - accuracy: 0.1000 - val_loss: 2.4001 - val_accuracy: 0.0891\n",
            "Epoch 78/1000\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.3988 - accuracy: 0.1000\n",
            "Epoch 78: val_loss improved from 2.40012 to 2.39998, saving model to ./model/78-2.4000.hdf5\n",
            "60/60 [==============================] - 2s 36ms/step - loss: 2.3988 - accuracy: 0.1000 - val_loss: 2.4000 - val_accuracy: 0.0891\n",
            "Epoch 79/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3983 - accuracy: 0.1017\n",
            "Epoch 79: val_loss improved from 2.39998 to 2.39978, saving model to ./model/79-2.3998.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3986 - accuracy: 0.1000 - val_loss: 2.3998 - val_accuracy: 0.0891\n",
            "Epoch 80/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3989 - accuracy: 0.1000\n",
            "Epoch 80: val_loss improved from 2.39978 to 2.39973, saving model to ./model/80-2.3997.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3985 - accuracy: 0.1000 - val_loss: 2.3997 - val_accuracy: 0.0891\n",
            "Epoch 81/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3987 - accuracy: 0.1000\n",
            "Epoch 81: val_loss improved from 2.39973 to 2.39954, saving model to ./model/81-2.3995.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3984 - accuracy: 0.1000 - val_loss: 2.3995 - val_accuracy: 0.0891\n",
            "Epoch 82/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3986 - accuracy: 0.1017\n",
            "Epoch 82: val_loss improved from 2.39954 to 2.39952, saving model to ./model/82-2.3995.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.3984 - accuracy: 0.1000 - val_loss: 2.3995 - val_accuracy: 0.0891\n",
            "Epoch 83/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3989 - accuracy: 0.0983\n",
            "Epoch 83: val_loss improved from 2.39952 to 2.39927, saving model to ./model/83-2.3993.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3982 - accuracy: 0.1000 - val_loss: 2.3993 - val_accuracy: 0.0891\n",
            "Epoch 84/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3984 - accuracy: 0.0983\n",
            "Epoch 84: val_loss improved from 2.39927 to 2.39926, saving model to ./model/84-2.3993.hdf5\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3981 - accuracy: 0.1000 - val_loss: 2.3993 - val_accuracy: 0.0891\n",
            "Epoch 85/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3970 - accuracy: 0.1017\n",
            "Epoch 85: val_loss improved from 2.39926 to 2.39911, saving model to ./model/85-2.3991.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.3979 - accuracy: 0.1000 - val_loss: 2.3991 - val_accuracy: 0.0891\n",
            "Epoch 86/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3983 - accuracy: 0.1017\n",
            "Epoch 86: val_loss improved from 2.39911 to 2.39893, saving model to ./model/86-2.3989.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3979 - accuracy: 0.1000 - val_loss: 2.3989 - val_accuracy: 0.0891\n",
            "Epoch 87/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3974 - accuracy: 0.1017\n",
            "Epoch 87: val_loss improved from 2.39893 to 2.39888, saving model to ./model/87-2.3989.hdf5\n",
            "60/60 [==============================] - 2s 36ms/step - loss: 2.3978 - accuracy: 0.1000 - val_loss: 2.3989 - val_accuracy: 0.0891\n",
            "Epoch 88/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3977 - accuracy: 0.1000\n",
            "Epoch 88: val_loss improved from 2.39888 to 2.39870, saving model to ./model/88-2.3987.hdf5\n",
            "60/60 [==============================] - 2s 35ms/step - loss: 2.3976 - accuracy: 0.1000 - val_loss: 2.3987 - val_accuracy: 0.0891\n",
            "Epoch 89/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3981 - accuracy: 0.0966\n",
            "Epoch 89: val_loss did not improve from 2.39870\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3976 - accuracy: 0.1000 - val_loss: 2.3988 - val_accuracy: 0.0891\n",
            "Epoch 90/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3978 - accuracy: 0.0983\n",
            "Epoch 90: val_loss improved from 2.39870 to 2.39849, saving model to ./model/90-2.3985.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3975 - accuracy: 0.1000 - val_loss: 2.3985 - val_accuracy: 0.0891\n",
            "Epoch 91/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3976 - accuracy: 0.1017\n",
            "Epoch 91: val_loss did not improve from 2.39849\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 2.3973 - accuracy: 0.1000 - val_loss: 2.3985 - val_accuracy: 0.0891\n",
            "Epoch 92/1000\n",
            "59/60 [============================>.] - ETA: 0s - loss: 2.3974 - accuracy: 0.1000\n",
            "Epoch 92: val_loss improved from 2.39849 to 2.39837, saving model to ./model/92-2.3984.hdf5\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 2.3973 - accuracy: 0.1000 - val_loss: 2.3984 - val_accuracy: 0.0891\n",
            "Epoch 93/1000\n",
            " 7/60 [==>...........................] - ETA: 1s - loss: 2.3974 - accuracy: 0.0714"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d1a08ad2e1eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                    callbacks=[early_stopping_callback,checkpointer])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "\n",
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "    \n",
        "modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                                                       batch_size=10,\n",
        "                                                       epochs=1000,\n",
        "                   verbose=1,\n",
        "                   callbacks=[early_stopping_callback,checkpointer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6ff015",
      "metadata": {
        "scrolled": true,
        "id": "ce6ff015",
        "outputId": "f22fbe81-52e8-40ef-b3f8-2aa1cee153c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf2onnx in c:\\users\\user\\anaconda3\\lib\\site-packages (1.9.3)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from tf2onnx) (2.26.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tf2onnx) (1.12)\n",
            "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tf2onnx) (1.20.3)\n",
            "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: onnx>=1.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tf2onnx) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (2.0.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U tf2onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d358851",
      "metadata": {
        "id": "0d358851",
        "outputId": "fcd82b43-ae7d-4040-b274-815b52c3d03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./onnx2/assets\n"
          ]
        }
      ],
      "source": [
        "model.save('./onnx2/', include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c695111f",
      "metadata": {
        "scrolled": true,
        "id": "c695111f",
        "outputId": "fcf9acc7-88d8-4564-dcd2-f8b16f14b292"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-10 12:29:58.750850: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
            "2022-05-10 12:29:58.750891: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "C:\\Users\\user\\anaconda3\\lib\\runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2022-05-10 12:30:04.746489: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
            "2022-05-10 12:30:04.747660: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
            "2022-05-10 12:30:04.748823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
            "2022-05-10 12:30:04.750070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
            "2022-05-10 12:30:04.751256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
            "2022-05-10 12:30:04.752410: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
            "2022-05-10 12:30:04.753545: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
            "2022-05-10 12:30:04.754687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
            "2022-05-10 12:30:04.754707: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-05-10 12:30:04.755030: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-05-10 12:30:04,755 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2022-05-10 12:30:05,196 - INFO - Signatures found in model: [serving_default].\n",
            "2022-05-10 12:30:05,196 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2022-05-10 12:30:05,197 - INFO - Output names: ['dense_1']\n",
            "2022-05-10 12:30:05.200399: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2022-05-10 12:30:05.200879: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
            "2022-05-10 12:30:05.204986: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-05-10 12:30:05.329323: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
            "  function_optimizer: Graph size after: 59 nodes (45), 78 edges (64), time = 5.588ms.\n",
            "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2022-05-10 12:30:05,517 - WARNING - From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2022-05-10 12:30:05.528334: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2022-05-10 12:30:05.528498: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
            "2022-05-10 12:30:05.529548: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-05-10 12:30:05.616931: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
            "  constant_folding: Graph size after: 39 nodes (-20), 58 edges (-20), time = 58.077ms.\n",
            "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
            "  constant_folding: Graph size after: 39 nodes (0), 58 edges (0), time = 13.025ms.\n",
            "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
            "\n",
            "2022-05-10 12:30:05,658 - INFO - Using tensorflow=2.8.0, onnx=1.11.0, tf2onnx=1.9.3/1190aa\n",
            "2022-05-10 12:30:05,658 - INFO - Using opset <onnx, 9>\n",
            "2022-05-10 12:30:06,058 - INFO - Computed 0 values for constant folding\n",
            "2022-05-10 12:30:06,388 - INFO - Optimizing ONNX model\n",
            "2022-05-10 12:30:06,774 - INFO - After optimization: Cast -1 (1->0), Const +1 (11->12), Identity -7 (7->0), Reshape +1 (1->2), Transpose -9 (10->1)\n",
            "2022-05-10 12:30:06,788 - INFO - \n",
            "2022-05-10 12:30:06,788 - INFO - Successfully converted TensorFlow model onnx2 to ONNX\n",
            "2022-05-10 12:30:06,788 - INFO - Model inputs: ['conv2d_1_input']\n",
            "2022-05-10 12:30:06,788 - INFO - Model outputs: ['dense_1']\n",
            "2022-05-10 12:30:06,789 - INFO - ONNX model is saved at model2.onnx\n"
          ]
        }
      ],
      "source": [
        "!python -m tf2onnx.convert --saved-model onnx2 --output model2.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1f40ba",
      "metadata": {
        "id": "0c1f40ba",
        "outputId": "d33e62d8-8830-465a-b061-2bf6493c93d8"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\reshape_layer.cpp:148: error: (-1:Backtrace) Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13216/214201339.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mblob\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_digit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxVal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminMaxLoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\reshape_layer.cpp:148: error: (-1:Backtrace) Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask'\n"
          ]
        }
      ],
      "source": [
        "def norm_digit(img):\n",
        "    # 무게 중심 좌표 추출\n",
        "    m = cv2.moments(img)\n",
        "    cx = m['m10'] / m['m00']\n",
        "    cy = m['m01'] / m['m00']\n",
        "    h, w = img.shape[:2]\n",
        "    \n",
        "    # affine 행렬 생성\n",
        "    aff = np.array([[1, 0, w/2 - (cx + 0.5)], [0, 1, h/2 - (cy + 0.5)]], \n",
        "                   dtype=np.float32)\n",
        "    \n",
        "    # warpAffine을 이용해 기하학 변환\n",
        "    dst = cv2.warpAffine(img, aff, (0, 0))\n",
        "    return dst\n",
        "\n",
        "def on_mouse(event, x,y, flags, params):\n",
        "    global oldx, oldy\n",
        "    if event==cv2.EVENT_LBUTTONDOWN:\n",
        "        oldx, oldy = x,y\n",
        "    elif event==cv2.EVENT_MOUSEMOVE:\n",
        "        if flags&cv2.EVENT_FLAG_LBUTTON:\n",
        "            cv2.line(img, (oldx,oldy), (x,y), 255, 20, cv2.LINE_AA)\n",
        "            cv2.imshow('image', img)\n",
        "            oldx,oldy = x,y\n",
        "            \n",
        "            \n",
        "net = cv2.dnn.readNet('./model2.onnx')\n",
        "if net.empty():\n",
        "    print('Network load failed')\n",
        "    sys.exit()\n",
        "\n",
        "img = np.zeros((150,150), np.uint8)\n",
        "cv2.imshow('image', img)\n",
        "cv2.setMouseCallback('image', on_mouse)\n",
        "\n",
        "while True:\n",
        "    key = cv2.waitKey()\n",
        "    if key==27:\n",
        "        break\n",
        "    elif key == ord(' '):\n",
        "        blob= cv2.dnn.blobFromImage(norm_digit(img), 1/255,(28,28))\n",
        "        net.setInput(blob)\n",
        "        prob = net.forward()\n",
        "        \n",
        "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
        "        digit = maxLoc[0]\n",
        "        print(f'{digit} ({maxVal*100:4.2f}%)')\n",
        "        img.fill(0)\n",
        "        cv2.imshow('image',img)\n",
        "    \n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "537c1e99",
      "metadata": {
        "scrolled": true,
        "id": "537c1e99",
        "outputId": "aebc4114-c8e8-4fd2-9264-c08c6f9bbe59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1.]]\n",
            "3 (100.00%)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "net = cv2.dnn.readNet('./model2.onnx')\n",
        "if net.empty():\n",
        "    print('Network load failed')\n",
        "    sys.exit()\n",
        "#\n",
        "img = cv2.imread('./stand_man/1 (41).jpeg', cv2.IMREAD_GRAYSCALE)\n",
        "blob= cv2.dnn.blobFromImage(img, 1, (150, 150), (104, 117, 123),\n",
        "                                swapRB = False)\n",
        "#out = net.forward()\n",
        "#print(out.shape)\n",
        "\n",
        "net.setInput(blob)\n",
        "prob = net.forward()\n",
        "print(prob)\n",
        "\n",
        "#img = cv.resize(img,None,fx=0.5,fy=0.5)\n",
        "_, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
        "digit = maxLoc[0]\n",
        "print(f'{digit} ({maxVal*100:4.2f}%)')\n",
        "\n",
        "cv2.imshow('img', img)\n",
        "\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72335da",
      "metadata": {
        "id": "c72335da",
        "outputId": "3a8d219a-285e-40a9-fcec-1c594c0d437e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 1)\n",
            "[[0.9999012]\n",
            " [0.9970293]\n",
            " [0.999435 ]]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 2 is out of bounds for axis 1 with size 1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21448/1990960918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 1"
          ]
        }
      ],
      "source": [
        "img = cv2.imread('./stand_man/1 (33).jpeg')\n",
        "net = cv2.dnn.readNet('./whole_model.onnx')\n",
        "if net.empty():\n",
        "    print('Network load failed')\n",
        "    sys.exit()\n",
        "#\n",
        "blob = cv2.dnn.blobFromImage(img, 1, (150,150), (104,177,123),\n",
        "                            swapRB=False)\n",
        "net.setInput(blob)\n",
        "out = net.forward()\n",
        "print(out.shape)\n",
        "print(out)\n",
        "detect = out[:,:]\n",
        "h,w = img.shape[:2]\n",
        "\n",
        "for i in range(detect.shape[0]):\n",
        "    confidence = detect[i, 2]\n",
        "    \n",
        "    if confidence > 0.5:\n",
        "        x1 = int(detect[i, 3]*w)\n",
        "        y1 = int(detect[i, 4]*h)\n",
        "        x2 = int(detect[i, 5]*w)\n",
        "        y2 = int(detect[i, 6]*h)\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,0,255), 2)\n",
        "        text = 'Face: {:.2f}%'.format(confidence*100)\n",
        "        cv2.putText(img,text, (x1,y1-2), cv2.FONT_HERSHEY_COMPLEX,\n",
        "                   0.8, (0,0,255), 1, cv2.LINE_AA)\n",
        "        \n",
        "cv2.imshow('img', img)\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16419e77",
      "metadata": {
        "id": "16419e77"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Project01.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}